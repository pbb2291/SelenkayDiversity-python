{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4cd2cf-c74d-4fb6-b6b6-fd9c151b7766",
   "metadata": {},
   "source": [
    "# Voxelize Lidar Data and Compute Metrics \n",
    "***Lidar-Notebooks***<br>\n",
    "Peter Boucher <br>\n",
    "2022/11/28 <br>\n",
    "\n",
    "<p>This is the second step in a 3 part process for 1) clipping las files with a set of polygons (1-ClipLasWithPolygonsforVoxels.ipynb); 2) voxeling lidar data, computing vegetation structure metrics, and outputting a pickle file (2-ProcessVoxelMetrics.ipynb); and 3) outputting the pixel and voxel grids of each metric as geotif or netcdf files for use in qgis and other software (3-OutputVoxelMetrics_Geotiff_NetCDF.ipynb). </p>\n",
    "\n",
    "#### Input files/folders: \n",
    "- a folder of las files to build a grid over\n",
    "    - Note: The input las files need to have a \"HeightAboveGround\" attribute for each point\n",
    "\n",
    "#### Output files:\n",
    "- a series of pickle files that store voxelized lidar metrics\n",
    "\n",
    "## Define User Inputs Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b252988-9a0e-4860-95af-e2fbfe0a36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import sys\n",
    "sys.path.append('/n/home02/pbb/scripts/halo-metadata-server/LabLidarScripts/bin/')\n",
    "from LabLidar_Functions import calccover, calcPercentileHeights, canopyLayerMetrics\n",
    "from LabLidar_Classes import Cloud\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "import laspy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.interpolate import interp1d\n",
    "import json\n",
    "\n",
    "# makes matplotlib plots big\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# # # USER INPUTS\n",
    "\n",
    "# Working directory\n",
    "wd = '/n/davies_lab/Users/pbb/SelenkayDiversity/data/PredictionSite01'\n",
    "\n",
    "# Input directory of las files to compute metrics over\n",
    "# ld = Path('/n/davies_lab/Users/pbb/SelenkayDiversity/data/PredictionTransect/las_clipped/')\n",
    "ld = Path(wd + '/las_clipped/')\n",
    "\n",
    "# make las inputs \n",
    "lasinputs = [l for l in ld.glob('*.las')]\n",
    "          \n",
    "# outdirectory for voxelized metric output pickle files\n",
    "od_metrics = Path(wd + '/metrics/')\n",
    "\n",
    "# EPSG code of the las files, as a string\n",
    "# Kruger is 32736 (WGS84 UTM 36S)\n",
    "# Mpala is 32637 (WGS84 UTM 37N)\n",
    "# Selenkay is 32737 (WGS84 UTM37S)\n",
    "epsg='32737'\n",
    "\n",
    "# Max height of voxel stacks \n",
    "# NOTE: Set this to be just above max height of your trees in meters.\n",
    "stackheight = 15\n",
    "\n",
    "# Horizontal Res of Grid (XY pixel size)\n",
    "xysize = 0.5\n",
    "\n",
    "# Vertical step size for metrics\n",
    "# NOTE: This defines the vertical bin size in meters (how \"thick\" each voxel is).\n",
    "verticalres = 0.5\n",
    "\n",
    "# Set the ground threshold in meters (i.e. below this height treat points as ground).\n",
    "# # # \n",
    "# EXPLANATION:\n",
    "# You can use groundthreshold to account for errors in relative accuracy\n",
    "# For example, if the rel. accuracy of ground is about 0.06 m (6 cm) between flightlines,\n",
    "# You could add in a voxel bin that extends from 0-0.06 m, \n",
    "# treating all points with a height in that range as ground points.\n",
    "# (so any hit below 0.06 m counts as ground).\n",
    "# If you prefer to use all points above 0 m, just set groundthres to 0.\n",
    "# Note: groundthreshold can also work with negative heights,\n",
    "# setting it to -0.05 for instance would treat points \n",
    "# with negative height values (between 0 and -5cm) as ground points.\n",
    "# # # \n",
    "groundthreshold = 0.05\n",
    "\n",
    "# height col\n",
    "heightcol = 'HeightAboveGround'\n",
    "\n",
    "# Set Complexity Metric Parameters\n",
    "# Note: Please leave these as their default values,\n",
    "# unless you have a strong reason to change them. \n",
    "# They have been parameterized for savanna environments. \n",
    "\n",
    "# Set method for calculation of peaks/layers (options = 'kde' or 'gauss1d')\n",
    "method = 'gauss1d'\n",
    "\n",
    "# Set smoothing sigma if using 'gauss1d\"\n",
    "sigma=1\n",
    "\n",
    "# set relative height for top of herb layer calc in canopyLayerMetrics\n",
    "rh = 1\n",
    "\n",
    "# # # END USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32de226d-8929-4396-8c11-e387280c99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for li in lasinputs:\n",
    "    if not li.exists():\n",
    "        print(f'Warning: {li} does not exist')\n",
    "\n",
    "if len(lasinputs) < 1:\n",
    "    print(f'Warning: Empty input directory- no las files found')\n",
    "    \n",
    "if not od_metrics.exists():\n",
    "    print('Warning: Output directory does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ee2ee-c43b-41ad-85d3-67355f029bc7",
   "metadata": {},
   "source": [
    "### Start Voxelizing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3716cf36-c506-4446-a346-f20553e3ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make voxel height bins\n",
    "# Calc Cover for height bins\n",
    "nbins = ((stackheight - 0) / verticalres) + 1\n",
    "heightbins = np.linspace(0, stackheight, int(nbins))\n",
    "\n",
    "if groundthreshold > 0:\n",
    "    # insert the groundthres into the array (right above 0)\n",
    "    heightbins = np.insert(heightbins, 1, groundthreshold)\n",
    "if groundthreshold < 0:\n",
    "    # insert the groundthres into the array (right below 0)\n",
    "    heightbins = np.insert(heightbins, 0, groundthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b933380-ff5d-4787-b642-19c81478f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for using parallel processing and calccover function \n",
    "# Notice that is calls lc as the first argument\n",
    "# need to write it this way in order to use concurrent futures parallel processing below\n",
    "def calccover_parallel(index, heightcol='HeightAboveGround'):\n",
    "\n",
    "    # make a True/False array \n",
    "    # for all points within the current grid cell\n",
    "    idx_bool = lc.grid_dict['idx_points'] == lc.grid_dict['idx_cells'][index]\n",
    "    \n",
    "    # Subset Points\n",
    "    p = lc.las.points[idx_bool]\n",
    "\n",
    "    # Get height array\n",
    "    # Note: this is slightly different from the \"heights\" output below\n",
    "    h = p[heightcol]\n",
    "    \n",
    "    # Remove high noise points above the canopy\n",
    "    h = h[h<=stackheight]\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Calculate Cover\n",
    "        cover = calccover(points=p,\n",
    "                          heightbins=heightbins, \n",
    "                          step=verticalres,\n",
    "                          groundthres=groundthreshold,\n",
    "                          heightcol=heightcol,\n",
    "                          hmax=stackheight)\n",
    "        \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Cover Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\")\n",
    "\n",
    "    try: \n",
    "        \n",
    "        # Calculate height statistics, and return an array of the point heights above groundthreshold\n",
    "        perc, heights = calcPercentileHeights(points=p,\n",
    "                                              groundthres=groundthreshold,\n",
    "                                              returnHeights=True,\n",
    "                                              heightcol=heightcol,\n",
    "                                              hmax=stackheight)\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Percentile Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Compute complexity metrics\n",
    "        complexity = canopyLayerMetrics(h=h,\n",
    "                                        hbins=heightbins,\n",
    "                                        method=method,\n",
    "                                        smoothsigma=sigma,\n",
    "                                        rel_height=rh,\n",
    "                                        groundthreshold=groundthreshold)\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Complexity Calc. - {e.__class__} for {lc.lasf}: \\n\")\n",
    "        print(f\"\\t{e}\\n\") \n",
    "\n",
    "    # Return cover dict, percentile dict, height array, and complexity metrics\n",
    "    return cover, perc, heights, complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830451c6-981f-4a65-9849-cf6c5f9a5f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded id_62 cloud, time elapsed: 0.05596446990966797\n",
      "\n",
      "Grid created, time elapsed: 0.07883644104003906\n",
      "\n",
      "Starting parallel processing of 15319 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 5.722124338150024\n",
      "\n",
      "76515 points gridded into 15319 0.5m pixels in 0.002047540413008796 hours!\n",
      "\n",
      "\tDone with processing id_62.\n",
      "\n",
      "Loaded id_119 cloud, time elapsed: 0.16076350212097168\n",
      "\n",
      "Grid created, time elapsed: 0.7305514812469482\n",
      "\n",
      "Starting parallel processing of 67804 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 30.432977199554443\n",
      "\n",
      "743448 points gridded into 67804 0.5m pixels in 0.010968270301818848 hours!\n",
      "\n",
      "\tDone with processing id_119.\n",
      "\n",
      "Loaded id_441 cloud, time elapsed: 0.4631011486053467\n",
      "\n",
      "Grid created, time elapsed: 0.016369104385375977\n",
      "\n",
      "Starting parallel processing of 7171 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 3.2607624530792236\n",
      "\n",
      "19019 points gridded into 7171 0.5m pixels in 0.0012458639012442696 hours!\n",
      "\n",
      "\tDone with processing id_441.\n",
      "\n",
      "Loaded id_209 cloud, time elapsed: 1.00473952293396\n",
      "\n",
      "Grid created, time elapsed: 18.309425830841064\n",
      "\n",
      "Starting parallel processing of 270400 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 295.34454250335693\n",
      "\n",
      "15008782 points gridded into 270400 0.5m pixels in 0.0972105018960105 hours!\n",
      "\n",
      "\tDone with processing id_209.\n",
      "\n",
      "Loaded id_218 cloud, time elapsed: 2.298194169998169\n",
      "\n",
      "Grid created, time elapsed: 18.03717803955078\n",
      "\n",
      "Starting parallel processing of 270399 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 297.09035086631775\n",
      "\n",
      "14859213 points gridded into 270399 0.5m pixels in 0.09821253286467659 hours!\n",
      "\n",
      "\tDone with processing id_218.\n",
      "\n",
      "Loaded id_102 cloud, time elapsed: 2.3341896533966064\n",
      "\n",
      "Grid created, time elapsed: 18.245118618011475\n",
      "\n",
      "Starting parallel processing of 270400 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 298.9140417575836\n",
      "\n",
      "15136072 points gridded into 270400 0.5m pixels in 0.09921080536312527 hours!\n",
      "\n",
      "\tDone with processing id_102.\n",
      "\n",
      "Loaded id_162 cloud, time elapsed: 2.214017391204834\n",
      "\n",
      "Grid created, time elapsed: 15.746229648590088\n",
      "\n",
      "Starting parallel processing of 270400 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 253.69592189788818\n",
      "\n",
      "12973442 points gridded into 270400 0.5m pixels in 0.08555463790893554 hours!\n",
      "\n",
      "\tDone with processing id_162.\n",
      "\n",
      "Loaded id_328 cloud, time elapsed: 2.269192695617676\n",
      "\n",
      "Grid created, time elapsed: 18.564199447631836\n",
      "\n",
      "Starting parallel processing of 270379 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 303.88748240470886\n",
      "\n",
      "15113281 points gridded into 270379 0.5m pixels in 0.10060537053479089 hours!\n",
      "\n",
      "\tDone with processing id_328.\n",
      "\n",
      "Loaded id_161 cloud, time elapsed: 3.812399387359619\n",
      "\n",
      "Grid created, time elapsed: 17.436397314071655\n",
      "\n",
      "Starting parallel processing of 270400 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 270.42802381515503\n",
      "\n",
      "13960039 points gridded into 270400 0.5m pixels in 0.0911243588394589 hours!\n",
      "\n",
      "\tDone with processing id_161.\n",
      "\n",
      "Loaded id_355 cloud, time elapsed: 2.4532663822174072\n",
      "\n",
      "Grid created, time elapsed: 16.584972858428955\n",
      "\n",
      "Starting parallel processing of 270400 pixels.\n",
      "\n",
      "Metrics computed, time elapsed: 275.21756768226624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lasf in lasinputs:\n",
    "\n",
    "    ### STEP 1: Load in Cloud \n",
    "    startproj = time.time()\n",
    "\n",
    "    # Make a las cloud class, and grid it\n",
    "    lc = Cloud(lasf=lasf,\n",
    "               gridsize=xysize,\n",
    "               vsize=verticalres,\n",
    "               heightcol=heightcol,\n",
    "               maxh=stackheight)\n",
    "\n",
    "    # get project string from file name for saving below\n",
    "    projstr = Path(lasf).name.split('.')[0]\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    loadtime = end - startproj\n",
    "    \n",
    "    print(f'Loaded {projstr} cloud, time elapsed: {loadtime}\\n')\n",
    "    \n",
    "    ### STEP 2: Make the grid\n",
    "    start = time.time()\n",
    "     \n",
    "    lc.makegrid()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    gridtime = end - start\n",
    "    \n",
    "    print(f'Grid created, time elapsed: {gridtime}\\n')\n",
    "    \n",
    "    ### STEP 2: Compute Cover, FHP, and Percentiles Metrics Over the Cloud's Grid\n",
    "    start = time.time()\n",
    "\n",
    "    # initialize dictionaries for output \n",
    "    lc.cover_dict = {}\n",
    "    lc.perc_dict = {}\n",
    "    lc.height_dict = {}\n",
    "    lc.complexity_dict = {}\n",
    "\n",
    "    # set the cell indices to loop over in parallel\n",
    "    indices = lc.grid_dict['idx_cells']\n",
    "    \n",
    "    numcells = len(lc.grid_dict['idx_cells'])\n",
    "    \n",
    "    print(f'Starting parallel processing of {numcells} pixels.\\n')\n",
    "    \n",
    "    # Assuming 2 seconds for each run, and 40 processes as once (should be a conservative estimate)\n",
    "    # print(f'\\tApproximate time to completion of {lasf.name}:{np.round(((numcells/40)*2)/36000, 3)} hrs')\n",
    "\n",
    "    ## Use concurrent futures to compute cover over each cell in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:\n",
    "        for cphc, x, y in zip(executor.map(calccover_parallel, indices),\n",
    "                                 lc.grid_dict['x_cells'],\n",
    "                                 lc.grid_dict['y_cells']):\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Stick the cover, perc, and heights inside the metrics dictionary\n",
    "                # with x and y location as tuple keys\n",
    "                lc.cover_dict[(x, y)] = cphc[0]\n",
    "                lc.perc_dict[(x, y)] = cphc[1]\n",
    "                lc.height_dict[(x, y)] = np.round(cphc[2], decimals=3)\n",
    "                lc.complexity_dict[(x, y)] = cphc[3]\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Saving metrics error - {e.__class__} for {lc.lasf} on pixel ({x}, {y}): \\n\")\n",
    "                print(f\"\\t{e}\\n\") \n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    metrictime = end - start\n",
    "    \n",
    "    print(f'Metrics computed, time elapsed: {metrictime}\\n')\n",
    "    \n",
    "    ### STEP 3: SAVE VOXEL & GRID METRICS\n",
    "    \n",
    "    # Save outputs as pickles\n",
    "    # \"Can't open a pickle you don't know\" - there can be malicious pickles, be wary.\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_covermetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.cover_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_percmetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.perc_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_heights.obj', 'wb') as of:\n",
    "        pickle.dump(lc.height_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'{od_metrics}/{projstr}_{xysize}mgrid_complexitymetrics.obj', 'wb') as of:\n",
    "        pickle.dump(lc.complexity_dict, of, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # DONE\n",
    "    endproj = time.time()\n",
    "    projtime = endproj - startproj\n",
    "    projhours = projtime/3600\n",
    "    \n",
    "    print(f'{lc.las.header.point_count} points gridded into {numcells} {xysize}m pixels in {projhours} hours!\\n')\n",
    "    \n",
    "    print(f'\\tDone with processing {projstr}.\\n')\n",
    "    \n",
    "    # Fill in log dictionary\n",
    "    log_dict = {'Project': str(projstr),\n",
    "                'lasf': str(lasf),\n",
    "                'XYsize':xysize,\n",
    "                'VerticalRes':verticalres,\n",
    "                'MaxH':stackheight,\n",
    "                'pcount': lc.las.header.point_count,\n",
    "                'TotalTime_hrs':projhours,\n",
    "                'TotalTime_s': projtime,\n",
    "                'LoadTime_s': loadtime,\n",
    "                'GridTime_s': gridtime,\n",
    "                'MetricTime_s': metrictime}\n",
    "    \n",
    "    with open(f'{od_metrics}/LOG_{projstr}_{xysize}mgrid.json', \"w\", encoding=\"utf-8\") as of:\n",
    "        # Save dictionary as json\n",
    "        json.dump(log_dict, of, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f34b72-3c7a-4b26-8dc9-6344edce46e5",
   "metadata": {},
   "source": [
    "#### DONE - Tests and Benchmarks Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b460c51-6c44-4876-9cdd-f11cb7d7071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Script for Benchmarking time\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# # set the cell indices to loop over in parallel\n",
    "# indices = lc.grid_dict['idx_cells'][0:1000]\n",
    "\n",
    "# t = []\n",
    "\n",
    "# ## Use concurrent futures to compute cover over each cell in parallel\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:\n",
    "#     for cphc, x, y in zip(executor.map(calccover_parallel, indices),\n",
    "#                              lc.grid_dict['x_cells'],\n",
    "#                              lc.grid_dict['y_cells']):\n",
    "\n",
    "#         try:\n",
    "            \n",
    "#             # Stick the cover, perc, and heights inside the metrics dictionary\n",
    "#             # with x and y location as tuple keys\n",
    "#             lc.cover_dict[(x, y)] = cphc[0]\n",
    "#             lc.perc_dict[(x, y)] = cphc[1]\n",
    "#             lc.height_dict[(x, y)] = np.round(cphc[2], decimals=3)\n",
    "#             lc.complexity_dict[(x, y)] = cphc[3]\n",
    "\n",
    "#         except Exception as e:\n",
    "\n",
    "#             print(f\"Saving metrics error - {e.__class__} for {lc.lasf} on pixel ({x}, {y}): \\n\")\n",
    "#             print(f\"\\t{e}\\n\") \n",
    "            \n",
    "# print(f'\\t1000 {xysize} m cells takes {time.time() - start} s')\n",
    "# # Note: 1000 0.5 m cells takes 43.7 seconds\n",
    "# So you can't run this over an entire 1 km tile (~4 million 0.5 m pixels)\n",
    "# that would take about 2 days per tile.\n",
    "# running at 1 m pixels takes about half the time (# Note: 1000 1 m cells takes 25.5 seconds)\n",
    "# you have to run it over a smaller section, \n",
    "# or downsize the resolution (that said, 1000 5 m cells takes 24.8 s, so there's not much gain)\n",
    "# either that, or make more of an effort to make the code efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Halo]",
   "language": "python",
   "name": "conda-env-.conda-Halo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
